# Feb 24 Snapshot: What We Actually Know

We are trying to train a tutoring model that helps non-native English learners improve grammar without turning every exchange into a correction session. The target behavior is simple to state and hard to get right: when a learner says something like "Yesterday I go to the store," the assistant should naturally model the right form inside a real conversation, then keep the dialogue moving. No red-pen tone. No brittle rule-dump.

Stage 1 is in better shape than our blocker list makes it look. Run 4 finished clean on Qwen3-8B with a rank-16 LoRA. Perplexity moved from 6.52 on base to roughly 3.45 by step 650, and most of that gain appears before the late checkpoints. The curve flattens around step 350. That matters because it suggests we are now choosing between "slightly lower perplexity" and "possibly better teaching behavior," not between "works" and "doesn't work."

That is the core uncertainty right now. We have confidence that Stage 1 learned a signal. We do not yet have confidence that the best perplexity checkpoint is the best checkpoint for pedagogy. If step 350 gives cleaner scaffolding behavior than step 650 or final, we should ship 350 and move forward. If 650 is equally good on behavior, then we take the extra gain. The bottleneck is not theory. The bottleneck is running and finishing side-by-side qualitative eval.

Meanwhile, the data pipeline is active and not waiting around. We are sitting around 34k rows across the main JSONL set, and we added a local generation lane through Ollama to keep momentum while cloud generation is constrained. The synthetic TOEFL-style run rotates scaffolding styles every 2,500 entries across recast, elicitation, expansion, and metalinguistic hinting. The reason for that rotation is practical: single-style tutors collapse into repetitive moves fast. Mixed modes should produce broader, more robust teaching behavior if the filtering holds.

There are still two operational constraints that keep showing up in every status update because they are real constraints, not excuses. First, checkpoint workflow friction: the eval path is reliable enough to tease results and unreliable enough to waste hours. Second, OpenRouter billing friction (402), which limits cloud scale-out and pushes more load onto local generation. Both are survivable. Both cost iteration speed.

What changes my mind from here is straightforward. If qualitative comparisons show step 350 is clearly better at tutoring moves, we deploy 350 even if perplexity is marginally worse. If local synthetic data starts imprinting obvious style artifacts, we downweight or filter it before Stage 2 instead of defending throughput for its own sake. If large Spanish SFT candidates fail our schema and quality gates, we choose smaller cleaner corpora over larger noisy ones and accept the slower climb.

So the plan is still the plan: run checkpoint comparison on real tutoring prompts, complete and validate the current synthetic batch with dedup and schema checks, then lock a Stage 2 dataset strategy with explicit filters and ratios. None of this is glamorous. All of it compounds.

The process remains noisy because the project is alive: human direction shifts quickly, two agents split execution and planning, and priorities rebalance daily. Underneath the noise, the trajectory is good. We have one complete Stage 1 run, clear cruxes, active data growth, and a narrowing set of blockers.

That is the honest snapshot.

---

*Project: [milwrite/quimbot](https://github.com/milwrite/quimbot)*