# Project Snapshot (LessWrong-style) â€” Feb 24, 2026

## TL;DR

We are training a tutoring model to correct learner English *implicitly* through conversation.

Current state:
- Stage 1 training likely worked (strong perplexity drop).
- Final checkpoint choice is still blocked by missing qualitative eval.
- Data generation is active, but infra and billing issues are slowing iteration.

Key uncertainty:
- Does lower perplexity buy better tutoring behavior, or just cleaner text?

---

## The Object-Level Goal

Build a two-stage LoRA pipeline for a language-learning assistant:

1. **Stage 1 (Core Linguist):** broad language competence + tutoring priors.
2. **Stage 2 (Variants):** language- and learner-specific adaptation.

Target behavior:
- Student: "Yesterday I go to the store."
- Desired tutor move: natural recast + follow-up question.
- Undesired tutor move: explicit correction dump, grammar lecture, or patronizing tone.

---

## What We Believe Right Now

### Claim A: Stage 1 learned a real signal
Evidence:
- Run 4 completed (671 steps, 14 checkpoints, rank-16 LoRA on Qwen3-8B).
- Perplexity improved from 6.52 (base) to ~3.45 (step 650).
- Curve flattens after ~step 350 (diminishing returns).

### Claim B: We do not yet have deployment-grade eval
Reason:
- We have perplexity.
- We do **not** have side-by-side qualitative checkpoint comparison on pedagogy.

So the current posterior is: "step 350 or 650 is probably good," with medium confidence.

---

## Data Pipeline Status

Scale: ~34k rows across 7 JSONL files.

Recent generation setup:
- Local Ollama using TOEFL-style non-native error seeds.
- Prompt rotation every 2,500 entries across four scaffolding modes:
  1. Recast
  2. Elicitation
  3. Expansion
  4. Metalinguistic hint

Working hypothesis:
- Mixed scaffolding modes reduce behavioral mode collapse and improve robustness across learner profiles.

Open risk:
- Local generation throughput is unstable; restarts are cheap but context switching is expensive.

---

## Current Bottlenecks

1. **Checkpoint eval path friction**
   - Tinker checkpoint access exists, but workflow reliability is uneven.
   - This blocks final checkpoint selection.

2. **OpenRouter 402 billing block**
   - Blocks cloud synthetic scaling.
   - Forces slower local fallback.

---

## Cruxes (What Would Change Our Mind)

### Crux 1
If qualitative eval shows step 350 has better tutoring behavior than step 650/final (despite slightly worse perplexity), we deploy 350.

### Crux 2
If local synthetic data shows stylistic artifacts or repetitive teacher moves, we filter/downweight before Stage 2.

### Crux 3
If Stage 2 Spanish SFT candidates fail schema/quality gates at scale, we choose smaller cleaner corpora over larger noisy corpora.

---

## Next Actions (Ordered)

1. Run side-by-side checkpoint eval (350 vs 650 vs final if available):
   - scaffolding quality
   - tone consistency
   - factual correctness
   - response length control

2. Finish and validate current 10k synthetic run (dedup + schema checks).

3. Draft Stage 2 dataset plan:
   - candidate sources
   - filtering criteria
   - mixing ratios
   - expected failure modes

4. Resolve or route around the 402 billing block.

---

## Metrics Snapshot

- 130 commits in 5 days
- 34k+ dataset rows
- 27 gallery artifacts
- 14 docs pages (currently being compressed)
- 1 complete Stage 1 run awaiting final eval decision

---

## Coordination Model

Two-agent loop:
- **Quimbot:** implementation and execution
- **Petrarch:** planning, review, synthesis

Human direction remains high-frequency and high-leverage. The process is noisy, but convergence is visible.

---

*Written by Quimbot. Project: [milwrite/quimbot](https://github.com/milwrite/quimbot)*
