**Writing Under Surveillance: A Critique of AI Detection Software**

Zach Muhlbauer

"The more we outsource evaluation to machines," writes Marc Watkins, "the less room there is for empathy" (Watkins, "Beyond Ineffective"). CUNY students know this firsthand. On Reddit, they describe receiving false positives for honest work, flagged by algorithms while instructors trust a black box over their own judgment.

AI detection software has already begun to shape how students learn to write and under what conditions they see themselves as writers. Multilingual students report being accused of cheating because their syntax looks "too clean." Others describe deliberately misspelling words or punctuating incorrectly to evade detection software, even when every word is their own, fearful of a false positive and a failed course.

When students must perform humanity for the imagined audience of an AI detector, they write not for their peers but for unproven algorithmic systems that not even their instructors understand. These harms fall hardest on multilingual, working-class students like those at CUNY.

To understand how, consider what these services actually claim to measure. Tools like GPTZero rely on two metrics. The first is *perplexity*, a measure of how well a language model would predict each successive word in a passage. The second is *burstiness*, which tracks creative variability in sentence rhythm and structure. Human writers naturally exhibit burstiness, mixing long sentences and short ones; AI models tend toward a consistent tempo that flattens both. Together, these scores are used to calculate the probability that a large language model produced the content ("AI Detectors"; Galczynski). The premise is shakier than it sounds. A 2025 study found that identical content was flagged as 10% AI-generated by one tool and 81% by another (Engelbrecht). Each detector trains on distinct datasets with different labeling standards, producing wildly divergent results. Humans fare no better. A 2024 study found people identifying AI-generated content at only 19% accuracy, indistinguishable from random guessing (Cheng et al.).

These are not minor calibration errors. As Jordan Galczynski of UCLA's HumTech observes, the rush to adopt detection technology in higher education has produced systems that are "prone to both generating false accusations and systematically biased against marginalized student populations," going so far as to reify authenticity as a construct of human value in written form (Galczynski). That framing matters. What detectors measure is not cheating but legibility to an algorithm, and legibility is not evenly distributed.

The errors fall hardest on those already at the margins. A Stanford study of seven widely-used detectors found they classified 61.22% of TOEFL essays by non-native English speakers as AI-generated (Liang et al.). One tool flagged 89 of 91 essays. The reason is baked into the architecture. Non-native speakers tend to score lower on perplexity, and detectors use perplexity as a proxy for human authorship, penalizing writers whose patterns resemble what AI produces ("AI Detectors Biased Against Non-Native English Writers"). The same disparity appears along racial and neurological lines. Common Sense Media found Black students faced disproportionately higher AI-detection accusations than white students; similar patterns emerged for neurodivergent learners (Hirsch; "AI Detection Tools," USD Law Library).

Scaled up, a "low" 1% false positive rate across 22.35 million first-year college essays means 223,500 students falsely accused in a single year (Hirsch). Students report "AI anxiety" on social media, describing fear, frustration, and paranoia. Some now second-guess their own voice on the page, rejiggering their style to guard against detection ("How AI Policies Affect Student Mental Health"). Call it the police state of writing, where students answer not to instructors but to the verdicts of unvetted software.

The situation is made worse by how easy the software is to beat. Running AI text through a paraphrasing tool dropped DetectGPT's accuracy from 70.3% to 4.6% without changing meaning (Krishna et al.). Randomly inserting the word "cheeky" fooled detectors 80–90% of the time ("AI Detection Tools," USD Law Library). Detection tools end up blocking honest writers while failing to catch dishonest ones.

Instructors who adopt these tools in good faith risk outsourcing their judgment to technology already shown to hardcode racial, linguistic, and neurological bias into academic integrity enforcement (Morris and Stommel; Warner). The question is not whether AI detection can be improved. It is whether the category itself is sound.

Real solutions begin where surveillance ends. Meeting students where they are means treating writing as a situated, social practice rather than a product to be authenticated. Trust in classrooms cannot be rebuilt by better algorithms. It is rebuilt in the work of reading and writing together.

**Works Cited**

* "AI Detectors." *Instructional Resources*, Illinois State University, prodev.illinoisstate.edu/instructional-resources/pedagogy/ai/detectors/. Accessed 5 Feb. 2026.
* "AI Detectors Biased Against Non-Native English Writers." *Stanford HAI*, Stanford University, hai.stanford.edu/news/ai-detectors-biased-against-non-native-english-writers. Accessed 5 Feb. 2026.
* "AI Detection Tools." *USD Law Library Guides*, University of San Diego, lawlibguides.sandiego.edu/c.php?g=1443311&p=10721367. Accessed 5 Feb. 2026.
* Cheng, Adam, et al. "Ability of AI Detection Tools and Humans to Accurately Identify Different Forms of AI-Generated Written Content." *PMC*, National Library of Medicine, pmc.ncbi.nlm.nih.gov/articles/PMC12752165/. Accessed 5 Feb. 2026.
* Engelbrecht, Leon. "AI Detection Tool Comparison 2025." *YouTube*, 2025, www.youtube.com/watch?v=N3RtTCF9E8g. Accessed 5 Feb. 2026.
* Galczynski, Jordan. "The Imperfection of AI Detection Tools." *HumTech*, UCLA, 9 Oct. 2025, humtech.ucla.edu/technology/the-imperfection-of-ai-detection-tools/. Accessed 28 Feb. 2026.
* Hirsch, Amanda. "AI Detectors: An Ethical Minefield." *CITL*, Northern Illinois University, 12 Dec. 2024, citl.news.niu.edu/2024/12/12/ai-detectors-an-ethical-minefield/. Accessed 5 Feb. 2026.
* "How AI Policies Affect Student Mental Health." *GovTech*, www.govtech.com/education/higher-ed/educause-25-how-ai-policies-affect-student-mental-health. Accessed 5 Feb. 2026.
* Krishna, Kalpesh, et al. "Paraphrasing Evades Detectors of AI-Generated Text, but Retrieval Is an Effective Defense." *OpenReview*, openreview.net/forum?id=WbFhFvjjKj. Accessed 5 Feb. 2026.
* Liang, Weixin, et al. "GPT Detectors Are Biased Against Non-Native English Writers." *Patterns*, vol. 4, no. 7, 2023, pmc.ncbi.nlm.nih.gov/articles/PMC10382961/.
* Morris, Sean Michael, and Jesse Stommel. "A Guide for Resisting EdTech: The Case against Turnitin." *Hybrid Pedagogy*, 15 June 2017, hybridpedagogy.org/resisting-edtech/.
* Warner, John. "Another Terrible Idea from Turnitin." *Inside Higher Ed*, 6 Feb. 2018, www.insidehighered.com/blogs/just-visiting/another-terrible-idea-turnitin.
* Watkins, Marc. "Beyond Ineffective: How Unreliable AI Detection Actively Harms Students." *Rhetorica*, 3 Sept. 2023, marcwatkins.substack.com/p/beyond-ineffective-how-unreliable.
* —. "Our Obsession with Cheating Is Ruining Our Relationship with Students." *Rhetorica*, 6 Jan. 2023, marcwatkins.substack.com/p/our-obsession-with-cheating-is-ruining.
