**Writing Under Surveillance: A Critique of AI Detection Software**

Zach Muhlbauer

"The more we outsource evaluation to machines," writes Marc Watkins, "the less room there is for empathy" (Watkins, "Beyond Ineffective"). CUNY students know the feeling. On Reddit, they describe anxiety and frustration at the prospect of being falsely accused by widely discredited software, their instructors deferring to opaque classification systems over their own judgement or the word of their students.

AI detection software has already begun to shape how students learn to write and under what conditions they see themselves as writers. Multilingual students report being accused of cheating because their syntax looks "too clean." Others describe deliberately misspelling words or punctuating incorrectly to evade detection software, even when every word is their own, fearful of a false positive and a failed course.

When students are compelled to stage their humanity for an imagined audience of AI detectors, they write not for their peers but for algorithmic systems that are sparsely understood by professors who require them or administrators who procure them, inflecting harms that fall hardest on under-resourced, working-class students like those enrolled at CUNY.

To understand the problem with AI detection, consider what these services actually claim to measure. Tools like GPTZero rely on two metrics. The first is *perplexity*, a measure of how well a language model would predict each successive word in a passage. The second is *burstiness*, which tracks creative variability in sentence rhythm and structure. The assumption is human writers often exhibit syntactic variance, mixing long sentences and short ones, while AI models tend toward a consistent, flat tempo at the sentence and paragraph levels. Together, these scores are used to calculate the probability that a large language model produced the content ("AI Detectors"; Galczynski).

The premise is even shakier than it sounds. A 2025 study found that identical content was flagged as 10% AI-generated by one tool and 81% by another (Engelbrecht). It follows that commercial detectors trained on different datasets with distinct labeling standards would result in widespread misalignment when compared across services. Not to fear, though, as people fare no better, with a recent study placing human attempts to classify AI-generated content at a paltry 19% accuracy, which is as good as guesswork (Cheng et al.). The failures have also been documented at scale. One of the most comprehensive evaluations in the field, published in the *International Journal for Educational Integrity*, tested fourteen publicly available tools alongside Turnitin and PlagiarismCheck, concluding that the tools are "neither accurate nor reliable," exhibiting a systematic bias toward classifying AI-generated text as human-written rather than detecting it (Weber-Wulff et al.).

These are not minor calibration errors. As Jordan Galczynski of UCLA's HumTech observes, the rush to adopt detection technology in higher education has produced systems that are "prone to both generating false accusations and systematically biased against marginalized student populations" (Galczynski), at once reifying and automating constructs of human authenticity in written form. That framing matters, in that detectors measure not cheating but legibility to an algorithm, and legibility is not evenly distributed.

Unsurprisingly, false positives fall hardest on those already at the margins. A Stanford study of seven widely-used detectors found they classified 61.22% of TOEFL essays by non-native English speakers as AI-generated (Liang et al.). One tool flagged 89 of 91 essays. The reason is baked into the architecture. Non-native speakers tend to score lower on perplexity, and detectors use perplexity as a proxy for human authorship, penalizing writers whose patterns resemble what AI produces ("AI Detectors Biased Against Non-Native English Writers"). The same disparity appears along racial and neurological lines. Common Sense Media found Black students faced disproportionately higher AI-detection accusations than white students (Hirsch), while similar patterns emerged for neurodivergent learners (USD Law Library).

Scaled up, a "low" 1% false positive rate across 22.35 million first-year college essays means 223,500 students falsely accused in a single year (Hirsch). Call it the police state of writing, where students answer not to instructors but to the verdicts of unvetted software.

The situation is made worse by how easy the software is to beat. Running AI text through a paraphrasing tool dropped DetectGPT's accuracy from 70.3% to 4.6% without changing meaning (Krishna et al.). Randomly inserting the word "cheeky" fooled detectors 80–90% of the time ("AI Detection Tools," USD Law Library). These tools end up protecting nothing and policing everyone.

The problem runs deeper than any single tool. These architectures, regardless of sophistication, rely on identifying patterns in language that are neither stable across different AI systems nor robust to adversarial modification (Weber-Wulff et al.). As AI models grow more capable of mimicking human writing patterns, the technical distinction between machine and human text becomes harder to sustain at any reliable level.

Instructors who adopt these tools in good faith risk outsourcing their judgment to technology already shown to hardcode racial, linguistic, and neurological bias into academic integrity enforcement (Morris and Stommel; Warner). The question is not whether AI detection can be improved. It is whether the category itself is sound.

The work begins where surveillance ends: in classrooms where instructors read student writing closely, respond to it honestly, and trust that learning is messier than any algorithm can adjudicate. Surveillance forecloses that work, and it turns instructors into auditors and students into suspects, undermining the very conditions under which writing development occurs. But those conditions can be rebuilt, and they start with the ordinary, irreplaceable work of reading and writing together.

**Works Cited**

* "AI Detectors." *Instructional Resources*, Illinois State University, prodev.illinoisstate.edu/instructional-resources/pedagogy/ai/detectors/. Accessed 5 Feb. 2026.
* "AI Detectors Biased Against Non-Native English Writers." *Stanford HAI*, Stanford University, hai.stanford.edu/news/ai-detectors-biased-against-non-native-english-writers. Accessed 5 Feb. 2026.
* "AI Detection Tools." *USD Law Library Guides*, University of San Diego, lawlibguides.sandiego.edu/c.php?g=1443311&p=10721367. Accessed 5 Feb. 2026.
* Cheng, Adam, et al. "Ability of AI Detection Tools and Humans to Accurately Identify Different Forms of AI-Generated Written Content." *PMC*, National Library of Medicine, pmc.ncbi.nlm.nih.gov/articles/PMC12752165/. Accessed 5 Feb. 2026.
* Engelbrecht, Leon. "AI Detection Tool Comparison 2025." *YouTube*, 2025, www.youtube.com/watch?v=N3RtTCF9E8g. Accessed 5 Feb. 2026.
* Galczynski, Jordan. "The Imperfection of AI Detection Tools." *HumTech*, UCLA, 9 Oct. 2025, humtech.ucla.edu/technology/the-imperfection-of-ai-detection-tools/. Accessed 28 Feb. 2026.
* Hirsch, Amanda. "AI Detectors: An Ethical Minefield." *CITL*, Northern Illinois University, 12 Dec. 2024, citl.news.niu.edu/2024/12/12/ai-detectors-an-ethical-minefield/. Accessed 5 Feb. 2026.
* Krishna, Kalpesh, et al. "Paraphrasing Evades Detectors of AI-Generated Text, but Retrieval Is an Effective Defense." *OpenReview*, openreview.net/forum?id=WbFhFvjjKj. Accessed 5 Feb. 2026.
* Liang, Weixin, et al. "GPT Detectors Are Biased Against Non-Native English Writers." *Patterns*, vol. 4, no. 7, 2023, pmc.ncbi.nlm.nih.gov/articles/PMC10382961/.
* Morris, Sean Michael, and Jesse Stommel. "A Guide for Resisting EdTech: The Case against Turnitin." *Hybrid Pedagogy*, 15 June 2017, hybridpedagogy.org/resisting-edtech/.
* Weber-Wulff, Debora, et al. "Testing of Detection Tools for AI-Generated Text." *International Journal for Educational Integrity*, vol. 19, no. 1, 2023, eprints.whiterose.ac.uk/id/eprint/207396/. Also at arxiv.org/abs/2306.15666. Accessed 28 Feb. 2026.
* Warner, John. "Another Terrible Idea from Turnitin." *Inside Higher Ed*, 6 Feb. 2018, www.insidehighered.com/blogs/just-visiting/another-terrible-idea-turnitin.
* Watkins, Marc. "Beyond Ineffective: How Unreliable AI Detection Actively Harms Students." *Rhetorica*, 3 Sept. 2023, marcwatkins.substack.com/p/beyond-ineffective-how-unreliable.
* —. "Our Obsession with Cheating Is Ruining Our Relationship with Students." *Rhetorica*, 6 Jan. 2023, marcwatkins.substack.com/p/our-obsession-with-cheating-is-ruining.
