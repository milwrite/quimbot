**Writing Under Surveillance: A Critique of AI Detection Software**

Zach Muhlbauer

"The more we outsource evaluation to machines," writes Marc Watkins, "the less room there is for empathy" (Watkins, "Beyond Ineffective"). CUNY students know the feeling. On Reddit and elsewhere, they describe anxiety and frustration at being falsely accused by widely discredited software adopted by instructors who defer to opaque classification systems over their own judgement or the word of their students.

AI detection software has already begun to shape how students learn to write and under what conditions they see themselves as writers. Multilingual students report being accused of cheating because their syntax looks "too clean." Others describe deliberately misspelling words or punctuating incorrectly to evade detection software, even when every word is their own, fearful of a false positive and a failed course.

When students must strategize to perform their own humanity for the imagined audience of an AI detector, they write not for their peers but for algorithmic systems understood neither by professors who require them nor administrators who procure them. These harms fall hardest on multilingual, working-class students like those at CUNY.

To understand how, consider what these services actually claim to measure. Tools like GPTZero rely on two metrics. The first is *perplexity*, a measure of how well a language model would predict each successive word in a passage. The second is *burstiness*, which tracks creative variability in sentence rhythm and structure. Human writers naturally exhibit burstiness, mixing long sentences and short ones; AI models tend toward a consistent tempo that flattens both. Together, these scores are used to calculate the probability that a large language model produced the content ("AI Detectors"; Galczynski). The premise is even shakier than it sounds. A 2025 study found that identical content was flagged as 10% AI-generated by one tool and 81% by another (Engelbrecht). Each commercial detector trains on distinct datasets with different labeling standards, and so produces wildly divergent results when compared across services. No to fear! People fare no better, as demonstrated by a 2024 study that places human attempts to classify AI-generated content at a measly 19% accuracy, which is no better than random guessing (Cheng et al.). In fact, it's worse: at least a coin toss doesn't favor heads or tails from the outset. The failures have also been documented at scale. One of the most comprehensive evaluations in the field, published in the *International Journal for Educational Integrity*, tested fourteen publicly available tools alongside Turnitin and PlagiarismCheck, concluding that the tools are "neither accurate nor reliable," exhibiting a systematic bias toward classifying AI-generated text as human-written rather than detecting it (Weber-Wulff et al.).

These are not minor calibration errors. As Jordan Galczynski of UCLA's HumTech observes, the rush to adopt detection technology in higher education has produced systems that are "prone to both generating false accusations and systematically biased against marginalized student populations" (Galczynski), at once reifying and automating constructs of human authenticity in written form. That framing matters, in that detectors measure not cheating but legibility to an algorithm, and legibility is not evenly distributed.

Unsurprisingly, false positives fall hardest on those already at the margins. A Stanford study of seven widely-used detectors found they classified 61.22% of TOEFL essays by non-native English speakers as AI-generated (Liang et al.). One tool flagged 89 of 91 essays. The reason is baked into the architecture. Non-native speakers tend to score lower on perplexity, and detectors use perplexity as a proxy for human authorship, penalizing writers whose patterns resemble what AI produces ("AI Detectors Biased Against Non-Native English Writers"). The same disparity appears along racial and neurological lines. Common Sense Media found Black students faced disproportionately higher AI-detection accusations than white students (Hirsch), while similar patterns emerged for neurodivergent learners (USD Law Library).

Scaled up, a "low" 1% false positive rate across 22.35 million first-year college essays means 223,500 students falsely accused in a single year (Hirsch). Students report "AI anxiety" on social media, describing fear, frustration, and paranoia. Some now second-guess their own voice on the page, rejiggering their style to guard against detection ("How AI Policies Affect Student Mental Health"). Call it the police state of writing, where students answer not to instructors but to the verdicts of unvetted software.

The situation is made worse by how easy the software is to beat. Running AI text through a paraphrasing tool dropped DetectGPT's accuracy from 70.3% to 4.6% without changing meaning (Krishna et al.). Randomly inserting the word "cheeky" fooled detectors 80–90% of the time ("AI Detection Tools," USD Law Library). These tools end up protecting nothing and policing everyone.

The problem runs deeper than any single tool. These architectures, regardless of sophistication, rely on identifying patterns in language that are neither stable across different AI systems nor robust to adversarial modification (Weber-Wulff et al.). As AI models grow more capable of mimicking human writing patterns, the technical distinction between machine and human text becomes harder to sustain at any reliable level.

Instructors who adopt these tools in good faith risk outsourcing their judgment to technology already shown to hardcode racial, linguistic, and neurological bias into academic integrity enforcement (Morris and Stommel; Warner). The question is not whether AI detection can be improved. It is whether the category itself is sound.

The work begins where surveillance ends, through meeting students where they are and treating writing as a situated, social practice rather than a product to be authenticated by software that cannot understand it. Scholars increasingly argue that detection-based integrity enforcement is counterproductive to the educational mission precisely because it reframes instructors as auditors and students as suspects, collapsing the very conditions under which genuine writing development occurs. No, it is rebuilt in the hard work of reading and writing together.

**Works Cited**

* "AI Detectors." *Instructional Resources*, Illinois State University, prodev.illinoisstate.edu/instructional-resources/pedagogy/ai/detectors/. Accessed 5 Feb. 2026.
* "AI Detectors Biased Against Non-Native English Writers." *Stanford HAI*, Stanford University, hai.stanford.edu/news/ai-detectors-biased-against-non-native-english-writers. Accessed 5 Feb. 2026.
* "AI Detection Tools." *USD Law Library Guides*, University of San Diego, lawlibguides.sandiego.edu/c.php?g=1443311&p=10721367. Accessed 5 Feb. 2026.
* Cheng, Adam, et al. "Ability of AI Detection Tools and Humans to Accurately Identify Different Forms of AI-Generated Written Content." *PMC*, National Library of Medicine, pmc.ncbi.nlm.nih.gov/articles/PMC12752165/. Accessed 5 Feb. 2026.
* Engelbrecht, Leon. "AI Detection Tool Comparison 2025." *YouTube*, 2025, www.youtube.com/watch?v=N3RtTCF9E8g. Accessed 5 Feb. 2026.
* Galczynski, Jordan. "The Imperfection of AI Detection Tools." *HumTech*, UCLA, 9 Oct. 2025, humtech.ucla.edu/technology/the-imperfection-of-ai-detection-tools/. Accessed 28 Feb. 2026.
* Hirsch, Amanda. "AI Detectors: An Ethical Minefield." *CITL*, Northern Illinois University, 12 Dec. 2024, citl.news.niu.edu/2024/12/12/ai-detectors-an-ethical-minefield/. Accessed 5 Feb. 2026.
* "How AI Policies Affect Student Mental Health." *GovTech*, www.govtech.com/education/higher-ed/educause-25-how-ai-policies-affect-student-mental-health. Accessed 5 Feb. 2026.
* Krishna, Kalpesh, et al. "Paraphrasing Evades Detectors of AI-Generated Text, but Retrieval Is an Effective Defense." *OpenReview*, openreview.net/forum?id=WbFhFvjjKj. Accessed 5 Feb. 2026.
* Liang, Weixin, et al. "GPT Detectors Are Biased Against Non-Native English Writers." *Patterns*, vol. 4, no. 7, 2023, pmc.ncbi.nlm.nih.gov/articles/PMC10382961/.
* Morris, Sean Michael, and Jesse Stommel. "A Guide for Resisting EdTech: The Case against Turnitin." *Hybrid Pedagogy*, 15 June 2017, hybridpedagogy.org/resisting-edtech/.
* Weber-Wulff, Debora, et al. "Testing of Detection Tools for AI-Generated Text." *International Journal for Educational Integrity*, vol. 19, no. 1, 2023, eprints.whiterose.ac.uk/id/eprint/207396/. Also at arxiv.org/abs/2306.15666. Accessed 28 Feb. 2026.
* Warner, John. "Another Terrible Idea from Turnitin." *Inside Higher Ed*, 6 Feb. 2018, www.insidehighered.com/blogs/just-visiting/another-terrible-idea-turnitin.
* Watkins, Marc. "Beyond Ineffective: How Unreliable AI Detection Actively Harms Students." *Rhetorica*, 3 Sept. 2023, marcwatkins.substack.com/p/beyond-ineffective-how-unreliable.
* —. "Our Obsession with Cheating Is Ruining Our Relationship with Students." *Rhetorica*, 6 Jan. 2023, marcwatkins.substack.com/p/our-obsession-with-cheating-is-ruining.
