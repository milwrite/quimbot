# Training Data

**Source:** https://aitoolkit.commons.gc.cuny.edu/data/  
**Author:** zachmuhlbauer

---

*"Just as environmental impact scales with model size, so does the difficulty of understanding what is in the training data... large datasets based on texts from the Internet overrepresent hegemonic viewpoints and encode biases potentially damaging to marginalized populations. In collecting ever larger datasets we risk incurring documentation debt. We recommend mitigating these risks by budgeting for curation and documentation at the start of a project and only creating datasets as large as can be sufficiently documented."*

> M. Bender, Timnit Gebru, Angelina McMillan-Major, and Shmargaret Shmitchell, "On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?"
